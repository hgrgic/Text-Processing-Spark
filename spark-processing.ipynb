{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport pyspark as spark\nfrom pyspark.sql import SparkSession\nimport re\nfrom pyspark.sql.types import ArrayType, StringType, FloatType\nimport struct\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["spark = SparkSession \\\n    .builder \\\n    .appName(\"Spark - Text Processing\") \\\n    .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["df = spark.read.csv(\"/FileStore/tables/hacker_news_sample.csv\", header=True)\nrawdata = df.select(\"text\", \"by\").filter(df['type'] == 'story').filter(df['text'] != \"null\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["rawdata.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: 53178</div>"]}}],"execution_count":4},{"cell_type":"code","source":["def cleanup_text(record):\n    text = record\n    words = text.split()\n    \n    # Default list of Stopwords\n    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n    u'can', 'cant', 'come', u'could', 'couldnt', \n    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n    u'each', \n    u'few', 'finally', u'for', u'from', u'further', \n    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n    u'just', \n    u'll', \n    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n    u'no', u'nor', u'not', u'now', \n    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n    u'r', u're', \n    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n    u'under', u'until', u'up', \n    u'very', \n    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n    \n    # Custom List of Stopwords - Add your own here\n    stopwords_custom = ['']\n    stopwords = stopwords_core + stopwords_custom\n    stopwords = [word.lower() for word in stopwords]    \n\n    text_out = [re.sub(r'<[^>]*>',' ',word) for word in words] # Remove html tags\n    text_out = [re.sub(r'http://\\S+|https://\\S','',word) for word in words] #remove valid links\n    text_out = [re.sub(r'http:\\S+|https://\\S+',' ',word) for word in words] #remove invalid links\n    \n    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n    return text_out\n\nudf_cleantext = udf(cleanup_text , ArrayType(StringType()))\nclean_text = rawdata.withColumn(\"words\", udf_cleantext(rawdata['text']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n\n#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n#featurizedData = hashingTF.transform(clean_text)\n\n# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \ncv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\ncvmodel = cv.fit(clean_text)\nfeaturizedData = cvmodel.transform(clean_text)\n\nvocab = cvmodel.vocabulary\nvocab_broadcast = sc.broadcast(vocab)\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData) # TFIDF"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# rescaledData.select('features').show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.clustering import LDA\n\nlda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n\nldamodel = lda.fit(rescaledData)\n\nldatopics = ldamodel.describeTopics()\n#ldatopics.show(25)\n\ndef map_termID_to_Word(termIndices):\n    words = []\n    for termID in termIndices:\n        words.append(vocab_broadcast.value[termID])\n    \n    return words\n\nudf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\nldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\nldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---------------------------------------------------------------------------------------------------+\ntopic|topic_desc                                                                                         |\n+-----+---------------------------------------------------------------------------------------------------+\n0    |[like, get, people, one, new, time, work, know, 2012, domain]                                      |\n1    |[like, get, people, work, time, know, one, ix27m, want, medical]                                   |\n2    |[like, get, time, one, people, work, know, free, good, ix27m]                                      |\n3    |[text, like, games, use, get, one, new, people, know, online]                                      |\n4    |[dvd, hair, music, car, player, india, mac, mobile, video, videos]                                 |\n5    |[iphone, application, like, development, web, app, get, time, people, use]                         |\n6    |[ipad, like, medical, get, web, one, time, people, new, use]                                       |\n7    |[like, web, one, get, use, people, time, know, hosting, work]                                      |\n8    |[ipad, like, get, one, people, app, use, time, know, new]                                          |\n9    |[seo, like, get, web, one, people, new, work, know, time]                                          |\n10   |[essay, writing, students, write, like, ix27m, work, good, get, people]                            |\n11   |[samsung, memory, like, camera, time, new, get, one, people, work]                                 |\n12   |[shoes, air, nike, free, sale, cheap, running, price, run, new]                                    |\n13   |[movie, watch, like, people, one, get, time, film, movies, online]                                 |\n14   |[insurance, online, designer, wedding, shopping, indian, designs, password, affordable, collection]|\n15   |[seo, like, get, people, time, work, know, one, want, ix27m]                                       |\n16   |[water, password, like, get, one, people, time, use, know, new]                                    |\n17   |[property, battery, investors, laptop, like, one, get, time, work, people]                         |\n18   |[desktop, yang, dan, wallpapers, plus, photos, hot, beautiful, download, images]                   |\n19   |[london, like, get, company, work, people, business, ix27m, time, startup]                         |\n20   |[like, get, people, one, time, know, web, new, work, want]                                         |\n21   |[estate, real, paper, like, get, one, new, web, know, people]                                      |\n22   |[like, work, people, get, ix27m, company, time, one, know, business]                               |\n23   |[like, get, people, app, one, time, web, use, new, know]                                           |\n24   |[pdf, like, work, files, net, get, time, people, ix27m, one]                                       |\n+-----+---------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["ldaResults = ldamodel.transform(rescaledData)\nldaResults.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\n                text|             by|               words|         rawFeatures|            features|   topicDistribution|\n+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\n&amp;gt;&lt;i&gt;which lead...|        coldtea|[gtiwhich, leads,...|(1000,[4,5,11,13,...|(1000,[4,5,11,13,...|[0.03177307781880...|\nI would like to p...|         etanol|[like, point, cou...|(1000,[0,1,15,17,...|(1000,[0,1,15,17,...|[0.04794108984711...|\n&lt;i&gt;Our msbuild im...|      Locke1689|[iour, msbuild, i...|(1000,[11,185,209...|(1000,[11,185,209...|[0.04059060017236...|\nNo matter how awf...|    miloshadzic|[matter, awful, i...|(1000,[2,14,17,24...|(1000,[2,14,17,24...|[0.03814808995895...|\nThe existence of ...|      salsakran|[existence, way, ...|(1000,[6,9,12,16,...|(1000,[6,9,12,16,...|[0.03921469113649...|\nThe actual Intern...|     paulsutter|[actual, internet...|(1000,[4,12,17,20...|(1000,[4,12,17,20...|[0.04318048618519...|\nI want to know ho...|        sitkack|[want, know, ants...|(1000,[17,20,90,9...|(1000,[17,20,90,9...|[0.04180151140018...|\nFrench is supreme...|    cjsthompson|[french, supremel...|(1000,[4,9,55,77,...|(1000,[4,9,55,77,...|[0.03844060651348...|\nI actually went f...|          evgen|[actually, went, ...|(1000,[22,29,33,4...|(1000,[22,29,33,4...|[0.03919991187477...|\nFirst impression ...|     adventured|[first, impressio...|(1000,[6,40,68,44...|(1000,[6,40,68,44...|[0.03874310177536...|\nIt&amp;#x27;s not exa...|      jhanschoo|[itx27s, exactly,...|(1000,[4,15,50,66...|(1000,[4,15,50,66...|[0.03748589276912...|\nThanks. I choose ...|       greggman|[thanks, choose, ...|(1000,[3,5,12,19,...|(1000,[3,5,12,19,...|[0.03542132446360...|\nI used Sybase ASE...|lobster_johnson|[used, sybase, as...|(1000,[7,14,31,41...|(1000,[7,14,31,41...|[0.03559449459484...|\n&amp;gt;They&amp;#x27;ve ...|   shrimp_emoji|[gttheyx27ve, tri...|(1000,[2,14,52,25...|(1000,[2,14,52,25...|[0.03671933493508...|\nIf there&#39;s a high...|        cturner|[theres, high, ch...|(1000,[6,32,77,17...|(1000,[6,32,77,17...|[0.04063224237200...|\n&#34;Actually the def...|      idlewords|[actually, defini...|(1000,[29,231,329...|(1000,[29,231,329...|[0.03972217960365...|\n&amp;#62; If the appl...|            eru|[application, bec...|(1000,[0,2,9,12,1...|(1000,[0,2,9,12,1...|[0.03551371668978...|\nLooks like the 10...|         pawadu|[looks, like, 10k...|(1000,[0,83,235,8...|(1000,[0,83,235,8...|[0.04006670737613...|\nDefinitely has th...|     napierzaza|[definitely, slan...|(1000,[0,1,24,52,...|(1000,[0,1,24,52,...|[0.04000943793024...|\nFunny I had just ...|        tdicola|[funny, switched,...|(1000,[33,46,65,1...|(1000,[33,46,65,1...|[0.03905103120181...|\n+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["def breakout_array(index_number, record):\n    vectorlist = record.tolist()\n    return vectorlist[index_number]\n\nudf_breakout_array = udf(breakout_array, FloatType())\n\n# Extract document weights for Topics 12 and 20\nenrichedData = ldaResults.withColumn(\"Topic_12\", udf_breakout_array(lit(12), ldaResults.topicDistribution)).withColumn(\"topic_20\", udf_breakout_array(lit(20), ldaResults.topicDistribution)) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["enrichedData.select(\"text\", \"by\", \"Topic_12\").sort(desc(\"Topic_12\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+----------+\n                text|       by|  Topic_12|\n+--------------------+---------+----------+\nAnd with this one...|    pilif|0.42901105|\n&#34;As I see it HATE...|friendzis|0.40850642|\n&#34;&amp;#62;&lt;i&gt;&#34;&#34;By Dec...| brudgers|0.38949075|\n&#34;&lt;i&gt;All the Linux...|    dkarl|0.38648403|\nIn the US I worke...|   furyg3|0.36013252|\n+--------------------+---------+----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# enrichedData.select(\"text\", \"Topic_12\").filter(enrichedData['by'] == 'karl_gluck').sort(desc(\"Topic_12\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------+\n                text|  Topic_12|\n+--------------------+----------+\n&#34;Hi everyone!  I&#39;...|0.50739044|\n+--------------------+----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["data = enrichedData.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1193872435447101&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>data <span class=\"ansi-blue-fg\">=</span> enrichedData<span class=\"ansi-blue-fg\">.</span>toPandas<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">toPandas</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2197</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2198</span>         <span class=\"ansi-red-fg\"># Below is toPandas without Arrow optimization.</span>\n<span class=\"ansi-green-fg\">-&gt; 2199</span><span class=\"ansi-red-fg\">         </span>pdf <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">.</span>from_records<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> columns<span class=\"ansi-blue-fg\">=</span>self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2200</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2201</span>         dtype <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-blue-fg\">}</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    547</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    548</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 549</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>collectToPython<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    550</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> BatchedSerializer<span class=\"ansi-blue-fg\">(</span>PickleSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    551</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o8747.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 910.0 failed 1 times, most recent failure: Lost task 0.0 in stage 910.0 (TID 1349, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2378)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:245)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:325)\n\tat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3358)\n\tat org.apache.spark.sql.Dataset$$anonfun$50.apply(Dataset.scala:3357)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3357)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["max_idx = []\nfor i in range(len(data2['topicDistribution'])):\n  tempHigh = -100000\n  tempHighIdx = -1\n  for j in range(len(data2['topicDistribution'][i])):\n    if data2['topicDistribution'][i][j] > tempHigh:\n      tempHigh = data2['topicDistribution'][i][j]\n      tempHighIdx = j\n  \n  max_idx.append(tempHighIdx)\n\ndata2['maxTopic'] = max_idx"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["data2['maxTopic'].value_counts()\nmostPopular = [1,4,0,15]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Visualization\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.colors as mcolors\nimport numpy as np\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = data2.loc[data2.maxTopic == mostPopular[i], :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.words]\n    \n    ax.hist(doc_lens, bins = 1000, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n    ax.set_ylabel('Number of Documents', color=cols[i])\n    ax.set_title('Topic: '+str(mostPopular[i]), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,1000,9))\nfig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"spark-processing","notebookId":2745142674641047},"nbformat":4,"nbformat_minor":0}
